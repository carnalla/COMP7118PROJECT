{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE PROJECT RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import relevant libraries for executing machine learning models and visualizations\n",
    "- Load data from previously generated file and parse into numeric arrays for selected features and class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_tree\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "data = pd.read_csv('TrainTestSample.csv')\n",
    "features = ['Average Minutes', 'NBA Experience', 'Off Days']\n",
    "X = data[['AVGmins', 'Experience', 'OffDays']].values\n",
    "Y = data['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import relevant packages for machine learning and evaluation metrics. \n",
    "- Generate empty lists and dataframes for storage of results following each iteration of training and testing. \n",
    "- Set up models with default parameters to establish baseline results and present evaluation metrics in sorted table.\n",
    "- Generate visualization of model accuracy resuls relative to one another with Mean ± Confidence Intervals for 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "\n",
    "DTACC = []\n",
    "RFACC = []\n",
    "GBCACC = []\n",
    "LRACC = []\n",
    "MLPACC = []\n",
    "XGBACC = []\n",
    "DTScores = pd.DataFrame(columns = ['Precision', 'Recall', 'F1', 'None'])\n",
    "GBCScores = pd.DataFrame(columns = ['Precision', 'Recall', 'F1', 'None'])\n",
    "RFScores = pd.DataFrame(columns = ['Precision', 'Recall', 'F1', 'None'])\n",
    "LRScores = pd.DataFrame(columns = ['Precision', 'Recall', 'F1', 'None'])\n",
    "MLPScores = pd.DataFrame(columns = ['Precision', 'Recall', 'F1', 'None'])\n",
    "XGBScores = pd.DataFrame(columns = ['Precision', 'Recall', 'F1', 'None'])\n",
    "\n",
    "for a in range(0, 100):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "    DT = tree.DecisionTreeClassifier()\n",
    "    DT.fit(X_train, y_train)\n",
    "    DTPred = DT.predict(X_test)\n",
    "    DTAccuracy = round(accuracy_score(y_test, DTPred) * 100, 2)\n",
    "    DTACC.append(DTAccuracy)\n",
    "    DTPerf = precision_recall_fscore_support(y_test, DTPred, average = 'binary')\n",
    "    DTScores.loc[len(DTScores)] = precision_recall_fscore_support(y_test, DTPred, average = 'binary')\n",
    "    \n",
    "    RF = RandomForestClassifier()\n",
    "    RF.fit(X_train, y_train)\n",
    "    RFPred = RF.predict(X_test)\n",
    "    RFAccuracy = round(accuracy_score(y_test, RFPred) * 100, 2)\n",
    "    RFACC.append(RFAccuracy)\n",
    "    RFPerf = precision_recall_fscore_support(y_test, RFPred, average = 'binary')\n",
    "    RFScores.loc[len(RFScores)] = precision_recall_fscore_support(y_test, RFPred, average = 'binary')\n",
    "    \n",
    "    GBC = GradientBoostingClassifier()\n",
    "    GBC.fit(X_train, y_train)\n",
    "    GBCPred = GBC.predict(X_test)\n",
    "    GBCAccuracy = round(accuracy_score(y_test, GBCPred) * 100, 2)\n",
    "    GBCACC.append(GBCAccuracy)\n",
    "    GBCPerf = precision_recall_fscore_support(y_test, GBCPred, average = 'binary')\n",
    "    GBCScores.loc[len(GBCScores)] = precision_recall_fscore_support(y_test, GBCPred, average = 'binary')\n",
    "    \n",
    "    LR = LogisticRegression()\n",
    "    LR.fit(X_train, y_train)\n",
    "    LRPred = LR.predict(X_test)\n",
    "    LRAccuracy = round(accuracy_score(y_test, LRPred) * 100, 2)\n",
    "    LRACC.append(LRAccuracy)\n",
    "    LRPerf = precision_recall_fscore_support(y_test, LRPred, average = 'binary')\n",
    "    LRScores.loc[len(LRScores)] = precision_recall_fscore_support(y_test, LRPred, average = 'binary')\n",
    "\n",
    "    MLP = MLPClassifier(max_iter = 2000)\n",
    "    MLP.fit(X_train, y_train)\n",
    "    MLPYPred = MLP.predict(X_test)\n",
    "    MLPAccuracy = round(accuracy_score(y_test, MLPYPred) * 100, 2)\n",
    "    MLPACC.append(MLPAccuracy)\n",
    "    MLPPerf = precision_recall_fscore_support(y_test, MLPYPred, average = 'binary')\n",
    "    MLPScores.loc[len(MLPScores)] = precision_recall_fscore_support(y_test, MLPYPred, average = 'binary')\n",
    "    \n",
    "    XGB = xgb.XGBClassifier(use_label_encoder=False)\n",
    "    XGB.fit(X_train, y_train, eval_metric='rmse')\n",
    "    XGBPred = XGB.predict(X_test)\n",
    "    XGBAccuracy = round(accuracy_score(y_test, XGBPred) * 100, 2)\n",
    "    XGBACC.append(XGBAccuracy)\n",
    "    XGBPerf = precision_recall_fscore_support(y_test, XGBPred, average = 'binary')\n",
    "    XGBScores.loc[len(XGBScores)] = precision_recall_fscore_support(y_test, XGBPred, average = 'binary')\n",
    "\n",
    "models = ['Decision Tree', 'Random Forest', 'Gradient Boost', 'Logistic Regression', 'Multilayer Perceptron', 'Extreme Gradient Boosting']\n",
    "shorts = [DTACC, RFACC, GBCACC, LRACC, MLPACC, XGBACC]\n",
    "scores = [DTScores, RFScores, GBCScores, LRScores, MLPScores, XGBScores]\n",
    "colNames = ['Model', 'Mean Accuracy', 'Std Accuracy', 'Lower95', 'Upper95', 'Mean Precision', 'Mean Recall', 'Mean F-1 Score']\n",
    "summary = pd.DataFrame(columns = colNames)\n",
    "\n",
    "for b in range(0,len(models)): \n",
    "    \n",
    "    name = models[b]\n",
    "    mean = round(np.mean(shorts[b]), 2)\n",
    "    stdv = round(np.std(shorts[b]), 2)\n",
    "    lowr = round(mean - 1.96 * stdv, 2)\n",
    "    uppr = round(mean + 1.96 * stdv, 2)\n",
    "    precision = round(np.mean(scores[b]['Precision']), 2)\n",
    "    recall = round(np.mean(scores[b]['Recall']), 2)\n",
    "    F1 = round(np.mean(scores[b]['F1']), 2)\n",
    "    row = [name, mean, stdv, lowr, uppr, precision, recall, F1]\n",
    "    \n",
    "    summary.loc[len(summary)] = row\n",
    "    \n",
    "summary = summary.sort_values('Mean Accuracy', ascending = False).reset_index(drop = True)\n",
    "plt.scatter(summary['Mean Accuracy'], summary['Model'], s = 50, linestyle = 'None')\n",
    "plt.errorbar(summary['Mean Accuracy'], summary['Model'], xerr = summary['Std Accuracy'] * 1.96, linestyle = 'None')\n",
    "plt.title('Model Performance: Mean ± 95% Confidence Interval', y = 1.1, fontsize = 15, fontweight = 'bold')\n",
    "plt.xlabel('Accuracy Score', fontsize = 12, fontweight = 'bold')\n",
    "plt.yticks(fontweight = 'bold')\n",
    "plt.xlim(50, 100)\n",
    "\n",
    "#plt.savefig('ModelPerf.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE SET UP TO RUN FOCUSED SECOND EXPERIMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Copied from body of code above with focus on decision tree and XGBoost classifiers. \n",
    "- Variable 'max_depth' left as 'a' from exploratory analysis concerning model depth and evaluation metrics. \n",
    "- -- Was previously set to '2' for purposes of the second experiment presented in 'results' of project report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTACC = []\n",
    "XGBACC = []\n",
    "DTScores = pd.DataFrame(columns = ['Precision', 'Recall', 'F1', 'None'])\n",
    "XGBScores = pd.DataFrame(columns = ['Precision', 'Recall', 'F1', 'None'])\n",
    "\n",
    "for a in range(1, 101):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "    DT = tree.DecisionTreeClassifier(max_depth = a)\n",
    "    DT.fit(X_train, y_train)\n",
    "    DTPred = DT.predict(X_test)\n",
    "    DTAccuracy = round(accuracy_score(y_test, DTPred) * 100, 2)\n",
    "    DTACC.append(DTAccuracy)\n",
    "    DTPerf = precision_recall_fscore_support(y_test, DTPred, average = 'binary')\n",
    "    DTScores.loc[len(DTScores)] = precision_recall_fscore_support(y_test, DTPred, average = 'binary')\n",
    "\n",
    "    XGB = xgb.XGBClassifier(use_label_encoder=False, max_depth = a)\n",
    "    XGB.fit(X_train, y_train, eval_metric='rmse')\n",
    "    XGBPred = XGB.predict(X_test)\n",
    "    XGBAccuracy = round(accuracy_score(y_test, XGBPred) * 100, 2)\n",
    "    XGBACC.append(XGBAccuracy)\n",
    "    XGBPerf = precision_recall_fscore_support(y_test, XGBPred, average = 'binary')\n",
    "    XGBScores.loc[len(XGBScores)] = precision_recall_fscore_support(y_test, XGBPred, average = 'binary')\n",
    "    \n",
    "models = ['Decision Tree', 'Extreme Gradient Boosting']\n",
    "shorts = [DTACC, RFACC, GBCACC, LRACC, MLPACC, XGBACC]\n",
    "scores = [DTScores, RFScores, GBCScores, LRScores, MLPScores, XGBScores]\n",
    "colNames = ['Model', 'Mean Accuracy', 'Std Accuracy', 'Lower95', 'Upper95', 'Mean Precision', 'Mean Recall', 'Mean F-1 Score']\n",
    "summary = pd.DataFrame(columns = colNames)\n",
    "\n",
    "for b in range(0,len(models)): \n",
    "    \n",
    "    name = models[b]\n",
    "    mean = round(np.mean(shorts[b]), 2)\n",
    "    stdv = round(np.std(shorts[b]), 2)\n",
    "    lowr = round(mean - 1.96 * stdv, 2)\n",
    "    uppr = round(mean + 1.96 * stdv, 2)\n",
    "    precision = round(np.mean(scores[b]['Precision']), 2)\n",
    "    recall = round(np.mean(scores[b]['Recall']), 2)\n",
    "    F1 = round(np.mean(scores[b]['F1']), 2)\n",
    "    row = [name, mean, stdv, lowr, uppr, precision, recall, F1]\n",
    "    \n",
    "    summary.loc[len(summary)] = row\n",
    "    \n",
    "summary = summary.sort_values('Mean Accuracy', ascending = False).reset_index(drop = True)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, 101))\n",
    "y1 = XGBACC\n",
    "y2 = DTACC\n",
    "\n",
    "n = 1\n",
    "\n",
    "plt.plot(x[::n], y1[::n], label = 'XGBoost')\n",
    "plt.plot(x[::n], y2[::n], label = 'Decision Tree')\n",
    "\n",
    "\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Figure 4: Accuracy by Depth of Model', y = 1.05, fontweight = 'bold', fontsize = 15)\n",
    "plt.ylabel('Accuracy (%)', fontweight = 'bold')\n",
    "plt.xlabel('Depth of Tree-Based Model', fontweight = 'bold')\n",
    "plt.xlim(0, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = tree.DecisionTreeClassifier(max_depth = 4)\n",
    "DT.fit(X_train, y_train)\n",
    "DTPred = DT.predict(X_test)\n",
    "DTAccuracy = round(accuracy_score(y_test, DTPred) * 100, 2)\n",
    "#DTACC.append(DTAccuracy)\n",
    "DTPerf = precision_recall_fscore_support(y_test, DTPred, average = 'binary')\n",
    "#DTScores.loc[len(DTScores)] = precision_recall_fscore_support(y_test, DTPred, average = 'binary')\n",
    "\n",
    "XGB = xgb.XGBClassifier(use_label_encoder=False, max_depth = 4)\n",
    "XGB.fit(X_train, y_train, eval_metric='rmse')\n",
    "XGBPred = XGB.predict(X_test)\n",
    "XGBAccuracy = round(accuracy_score(y_test, XGBPred) * 100, 2)\n",
    "#XGBACC.append(XGBAccuracy)\n",
    "XGBPerf = precision_recall_fscore_support(y_test, XGBPred, average = 'binary')\n",
    "#XGBScores.loc[len(XGBScores)] = precision_recall_fscore_support(y_test, XGBPred, average = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE REST OF THIS NOTEBOOK IS FIGURES FOR USE IN PROJECT REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, XGBPred)\n",
    "cmp = sns.heatmap(cm, annot = True, fmt = 'g', cmap = 'Blues', cbar = False, annot_kws={\"fontsize\":12})\n",
    "cmp.set_xlabel('Actual Class', fontsize = 15, fontweight = 'bold', labelpad = 12)\n",
    "cmp.set_ylabel('Predicted Class', fontsize = 15, fontweight = 'bold', labelpad = 12)\n",
    "cmp.set_title('Confusion Matrix for XGBoost Model', fontsize = 20, fontweight = 'bold', y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = dtreeviz(DT, X, Y,\n",
    "                target_name=\"Class\",\n",
    "                feature_names=features,\n",
    "                class_names= ['Injured', 'Control'])\n",
    "\n",
    "viz\n",
    "#viz.save(\"RegDTree3.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(XGB, rankdir='LR', fmap = 'feature_map.txt')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(150, 100)\n",
    "fig.savefig('XGBTree3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 12))\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "injData = data[data['Class'] == 1]\n",
    "conData = data[data['Class'] == 0]\n",
    "\n",
    "x1 = injData['Experience']\n",
    "y1 = injData['AVGmins']\n",
    "z1 = injData['OffDays']\n",
    "\n",
    "x2 = conData['Experience']\n",
    "y2 = conData['AVGmins']\n",
    "z2 = conData['OffDays']\n",
    "\n",
    "cat = data.groupby('Class')\n",
    "\n",
    "\n",
    "#color = ['blue' if l == 0 else 'orange' for l in data['Class']]\n",
    "\n",
    "#ax.scatter(x, y, z, label = data['Class'], c = color, s = 60) \n",
    "\n",
    "ax.scatter(x1, y1, z1, c = 'orange', s = 60, label = 'Injured')\n",
    "ax.scatter(x2, y2, z2, c = 'blue', s = 60, label = 'Control') \n",
    "    \n",
    "ax.view_init(30, 30)\n",
    "\n",
    "ax.set_title('Injury Class Using Three Predictors', y= 1.05, fontsize = 25, fontweight = 'bold')\n",
    "ax.set_xlabel('Years of NBA Experience', fontsize = 15, labelpad = 20)\n",
    "ax.set_ylabel('Average Minutes Per Game', fontsize = 15, labelpad = 20)\n",
    "ax.set_zlabel('Rest Days in Last 21 Days', fontsize = 15, labelpad = 20)\n",
    "ax.legend(fontsize = 15, loc=1, bbox_to_anchor=(0.1,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import graphviz\n",
    "#from xgboost import plot_tree\n",
    "\n",
    "#plt.figure(figsize = (50, 50))\n",
    "#plot_tree(XGB, fontsize = 20)\n",
    "#plt.show()\n",
    "\n",
    "plot_tree(XGB, rankdir='LR')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(150, 100)\n",
    "fig.savefig('XGBTree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dtreeviz\n",
    "\n",
    "from dtreeviz.trees import dtreeviz \n",
    "\n",
    "viz = dtreeviz(DT, X, Y,\n",
    "                target_name=\"Class\",\n",
    "                feature_names=features,\n",
    "                class_names= ['Injured', 'Control'])\n",
    "\n",
    "viz\n",
    "viz.save(\"RegDTree.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(XGB, rankdir='LR')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(150, 100)\n",
    "#fig.savefig('XGBTree2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData = data[['AVGmins', 'Experience', 'OffDays', 'Class']]\n",
    "\n",
    "plotDataMeans = plotData.groupby('Class').mean()\n",
    "plotDataStdvs = plotData.groupby('Class').std()\n",
    "\n",
    "#Injured = plotData[plotData['Class'] == 1]\n",
    "#Injured = Injured.drop('Class', axis = 1)\n",
    "#Control = plotData[plotData['Class'] == 0]\n",
    "#Control = Control.drop('Class', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = plotDataMeans.columns\n",
    "\n",
    "y1 = plotDataMeans.iloc[0]\n",
    "yerr1 = plotDataStdvs.iloc[0]\n",
    "\n",
    "y2 = plotDataMeans.iloc[1]\n",
    "yerr2 = plotDataStdvs.iloc[1]\n",
    "\n",
    "z = np.arange(len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotDataMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(z - 0.2, y1, 0.4, color = 'blue')\n",
    "plt.errorbar(z - 0.2, y1, yerr = yerr1, fmt = \"o\", color = 'black')\n",
    "plt.bar(z + 0.2, y2, 0.4, color = 'orange')\n",
    "plt.errorbar(z + 0.2, y2, yerr = yerr2, fmt = \"o\", color = 'black')\n",
    "plt.xticks(z, x, fontsize = 12, fontweight = 'bold')\n",
    "plt.yticks(fontsize = 10, fontweight = 'bold')\n",
    "plt.title('Qualitative Between-Group Comparison', y = 1.05, fontsize = 15, fontweight = 'bold')\n",
    "\n",
    "colors = {'Controls':'blue', 'Injured':'orange'}         \n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "plt.legend(handles, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
